./run-example SparkPi
...
5/06/16 17:56:14 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 775 ms on localhost (2/2)
15/06/16 17:56:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/06/16 17:56:14 INFO DAGScheduler: Stage 0 (reduce at SparkPi.scala:35) finished in 0.816 s
15/06/16 17:56:14 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:35, took 1.119467 s
Pi is roughly 3.13872
...
./spark-submit ../examples/src/main/python/pi.py
15/06/16 18:07:14 INFO PythonRDD: Times: total = 713, boot = 564, init = 16, finish = 133
15/06/16 18:07:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 701 bytes result sent to driver
15/06/16 18:07:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 941 ms on localhost (1/2)
15/06/16 18:07:14 INFO PythonRDD: Times: total = 811, boot = 580, init = 70, finish = 161
15/06/16 18:07:14 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 701 bytes result sent to driver
15/06/16 18:07:14 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 997 ms on localhost (2/2)
15/06/16 18:07:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/06/16 18:07:14 INFO DAGScheduler: Stage 0 (reduce at /usr/local/spark/bin/../examples/src/main/python/pi.py:38) finished in 1.038 s
15/06/16 18:07:14 INFO DAGScheduler: Job 0 finished: reduce at /usr/local/spark/bin/../examples/src/main/python/pi.py:38, took 1.280920 s
Pi is roughly 3.142720

scala> val textFile = sc.textFile("README.md")
15/06/16 18:15:40 INFO MemoryStore: ensureFreeSpace(180608) called with curMem=0, maxMem=278019440
15/06/16 18:15:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 176.4 KB, free 265.0 MB)
15/06/16 18:15:40 INFO MemoryStore: ensureFreeSpace(25432) called with curMem=180608, maxMem=278019440
15/06/16 18:15:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.8 KB, free 264.9 MB)
15/06/16 18:15:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:49634 (size: 24.8 KB, free: 265.1 MB)
15/06/16 18:15:40 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/06/16 18:15:40 INFO SparkContext: Created broadcast 0 from textFile at <console>:21
textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:21

scala> textFile.count()
15/06/16 18:15:47 INFO FileInputFormat: Total input paths to process : 1
15/06/16 18:15:47 INFO SparkContext: Starting job: count at <console>:24
15/06/16 18:15:47 INFO DAGScheduler: Got job 0 (count at <console>:24) with 2 output partitions (allowLocal=false)
15/06/16 18:15:47 INFO DAGScheduler: Final stage: Stage 0(count at <console>:24)
15/06/16 18:15:47 INFO DAGScheduler: Parents of final stage: List()
15/06/16 18:15:47 INFO DAGScheduler: Missing parents: List()
15/06/16 18:15:47 INFO DAGScheduler: Submitting Stage 0 (README.md MapPartitionsRDD[1] at textFile at <console>:21), which has no missing parents
15/06/16 18:15:47 INFO MemoryStore: ensureFreeSpace(2632) called with curMem=206040, maxMem=278019440
15/06/16 18:15:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 2.6 KB, free 264.9 MB)
15/06/16 18:15:47 INFO MemoryStore: ensureFreeSpace(1923) called with curMem=208672, maxMem=278019440
15/06/16 18:15:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1923.0 B, free 264.9 MB)
15/06/16 18:15:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:49634 (size: 1923.0 B, free: 265.1 MB)
15/06/16 18:15:47 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/06/16 18:15:47 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:839
15/06/16 18:15:47 INFO DAGScheduler: Submitting 2 missing tasks from Stage 0 (README.md MapPartitionsRDD[1] at textFile at <console>:21)
15/06/16 18:15:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
15/06/16 18:15:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1295 bytes)
15/06/16 18:15:47 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1295 bytes)
15/06/16 18:15:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/06/16 18:15:47 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
15/06/16 18:15:47 INFO HadoopRDD: Input split: file:/usr/local/spark/README.md:1814+1815
15/06/16 18:15:47 INFO HadoopRDD: Input split: file:/usr/local/spark/README.md:0+1814
15/06/16 18:15:47 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/06/16 18:15:47 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/06/16 18:15:47 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/06/16 18:15:47 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/06/16 18:15:47 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/06/16 18:15:47 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1830 bytes result sent to driver
15/06/16 18:15:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1830 bytes result sent to driver
15/06/16 18:15:47 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 132 ms on localhost (1/2)
15/06/16 18:15:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 163 ms on localhost (2/2)
15/06/16 18:15:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/06/16 18:15:47 INFO DAGScheduler: Stage 0 (count at <console>:24) finished in 0.183 s
15/06/16 18:15:47 INFO DAGScheduler: Job 0 finished: count at <console>:24, took 0.300363 s
res0: Long = 98

scala> textFile.first()
15/06/16 18:16:49 INFO SparkContext: Starting job: first at <console>:24
15/06/16 18:16:49 INFO DAGScheduler: Got job 1 (first at <console>:24) with 1 output partitions (allowLocal=true)
15/06/16 18:16:49 INFO DAGScheduler: Final stage: Stage 1(first at <console>:24)
15/06/16 18:16:49 INFO DAGScheduler: Parents of final stage: List()
15/06/16 18:16:49 INFO DAGScheduler: Missing parents: List()
15/06/16 18:16:49 INFO DAGScheduler: Submitting Stage 1 (README.md MapPartitionsRDD[1] at textFile at <console>:21), which has no missing parents
15/06/16 18:16:49 INFO MemoryStore: ensureFreeSpace(2656) called with curMem=210595, maxMem=278019440
15/06/16 18:16:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.6 KB, free 264.9 MB)
15/06/16 18:16:49 INFO MemoryStore: ensureFreeSpace(1945) called with curMem=213251, maxMem=278019440
15/06/16 18:16:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1945.0 B, free 264.9 MB)
15/06/16 18:16:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:49634 (size: 1945.0 B, free: 265.1 MB)
15/06/16 18:16:49 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/06/16 18:16:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:839
15/06/16 18:16:49 INFO DAGScheduler: Submitting 1 missing tasks from Stage 1 (README.md MapPartitionsRDD[1] at textFile at <console>:21)
15/06/16 18:16:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
15/06/16 18:16:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, PROCESS_LOCAL, 1295 bytes)
15/06/16 18:16:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
15/06/16 18:16:49 INFO HadoopRDD: Input split: file:/usr/local/spark/README.md:0+1814
15/06/16 18:16:49 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1809 bytes result sent to driver
15/06/16 18:16:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 10 ms on localhost (1/1)
15/06/16 18:16:49 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/06/16 18:16:49 INFO DAGScheduler: Stage 1 (first at <console>:24) finished in 0.011 s
15/06/16 18:16:49 INFO DAGScheduler: Job 1 finished: first at <console>:24, took 0.020127 s
res1: String = # Apache Spark

scala> val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:23

scala> linesWithSpark.count()
15/06/16 18:17:56 INFO SparkContext: Starting job: count at <console>:26
15/06/16 18:17:56 INFO DAGScheduler: Got job 2 (count at <console>:26) with 2 output partitions (allowLocal=false)
15/06/16 18:17:56 INFO DAGScheduler: Final stage: Stage 2(count at <console>:26)
15/06/16 18:17:56 INFO DAGScheduler: Parents of final stage: List()
15/06/16 18:17:56 INFO DAGScheduler: Missing parents: List()
15/06/16 18:17:56 INFO DAGScheduler: Submitting Stage 2 (MapPartitionsRDD[2] at filter at <console>:23), which has no missing parents
15/06/16 18:17:56 INFO MemoryStore: ensureFreeSpace(2840) called with curMem=215196, maxMem=278019440
15/06/16 18:17:56 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.8 KB, free 264.9 MB)
15/06/16 18:17:56 INFO MemoryStore: ensureFreeSpace(2029) called with curMem=218036, maxMem=278019440
15/06/16 18:17:56 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2029.0 B, free 264.9 MB)
15/06/16 18:17:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:49634 (size: 2029.0 B, free: 265.1 MB)
15/06/16 18:17:56 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0
15/06/16 18:17:56 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:839
15/06/16 18:17:56 INFO DAGScheduler: Submitting 2 missing tasks from Stage 2 (MapPartitionsRDD[2] at filter at <console>:23)
15/06/16 18:17:56 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
15/06/16 18:17:56 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, localhost, PROCESS_LOCAL, 1295 bytes)
15/06/16 18:17:56 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, localhost, PROCESS_LOCAL, 1295 bytes)
15/06/16 18:17:56 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)
15/06/16 18:17:56 INFO HadoopRDD: Input split: file:/usr/local/spark/README.md:0+1814
15/06/16 18:17:56 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 1830 bytes result sent to driver
15/06/16 18:17:56 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)
15/06/16 18:17:56 INFO HadoopRDD: Input split: file:/usr/local/spark/README.md:1814+1815
15/06/16 18:17:56 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 1830 bytes result sent to driver
15/06/16 18:17:56 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 18 ms on localhost (1/2)
15/06/16 18:17:56 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 21 ms on localhost (2/2)
15/06/16 18:17:56 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/06/16 18:17:56 INFO DAGScheduler: Stage 2 (count at <console>:26) finished in 0.022 s
15/06/16 18:17:56 INFO DAGScheduler: Job 2 finished: count at <console>:26, took 0.032054 s
res2: Long = 19

scala> Stopping spark context.
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/06/16 18:18:01 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/06/16 18:18:01 INFO SparkUI: Stopped Spark web UI at http://spark1:4040
15/06/16 18:18:01 INFO DAGScheduler: Stopping DAGScheduler
15/06/16 18:18:01 INFO MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!
15/06/16 18:18:01 INFO MemoryStore: MemoryStore cleared
15/06/16 18:18:01 INFO BlockManager: BlockManager stopped
15/06/16 18:18:01 INFO BlockManagerMaster: BlockManagerMaster stopped
15/06/16 18:18:01 INFO SparkContext: Successfully stopped SparkContext
15/06/16 18:18:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorActor: OutputCommitCoordinator stopped!
15/06/16 18:18:01 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/06/16 18:18:01 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
15/06/16 18:18:01 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.

[root@spark1 spark]# ./bin/spark-submit --class "SimpleApp" --master spark://spark1:7077 ./target/scala-2.10/simple-project_2.10-1.0.jar 
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/06/16 18:36:48 INFO SparkContext: Running Spark version 1.3.1
15/06/16 18:36:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/06/16 18:36:48 INFO SecurityManager: Changing view acls to: root
15/06/16 18:36:48 INFO SecurityManager: Changing modify acls to: root
15/06/16 18:36:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
15/06/16 18:36:49 INFO Slf4jLogger: Slf4jLogger started
15/06/16 18:36:49 INFO Remoting: Starting remoting
15/06/16 18:36:49 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@spark1:57860]
15/06/16 18:36:49 INFO Utils: Successfully started service 'sparkDriver' on port 57860.
15/06/16 18:36:49 INFO SparkEnv: Registering MapOutputTracker
15/06/16 18:36:49 INFO SparkEnv: Registering BlockManagerMaster
15/06/16 18:36:49 INFO DiskBlockManager: Created local directory at /tmp/spark-14f19f60-42ac-488c-844d-c457cbc51ff9/blockmgr-5e1b1dad-9b82-4e61-8e03-afe605442043
15/06/16 18:36:49 INFO MemoryStore: MemoryStore started with capacity 265.1 MB
15/06/16 18:36:49 INFO HttpFileServer: HTTP File server directory is /tmp/spark-53edfb72-0daa-49ff-9389-353e1e3a5d5a/httpd-27911539-dfef-4ff7-840d-7ed93d94e50b
15/06/16 18:36:49 INFO HttpServer: Starting HTTP Server
15/06/16 18:36:49 INFO Server: jetty-8.y.z-SNAPSHOT
15/06/16 18:36:49 INFO AbstractConnector: Started SocketConnector@0.0.0.0:56925
15/06/16 18:36:49 INFO Utils: Successfully started service 'HTTP file server' on port 56925.
15/06/16 18:36:49 INFO SparkEnv: Registering OutputCommitCoordinator
15/06/16 18:36:49 INFO Server: jetty-8.y.z-SNAPSHOT
15/06/16 18:36:49 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/06/16 18:36:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/06/16 18:36:49 INFO SparkUI: Started SparkUI at http://spark1:4040
15/06/16 18:36:49 INFO SparkContext: Added JAR file:/usr/local/spark/./target/scala-2.10/simple-project_2.10-1.0.jar at http://10.55.129.197:56925/jars/simple-project_2.10-1.0.jar with timestamp 1434497809767
15/06/16 18:36:49 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@spark1:7077/user/Master...
15/06/16 18:36:50 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150616183650-0000
15/06/16 18:36:50 INFO AppClient$ClientActor: Executor added: app-20150616183650-0000/0 on worker-20150616174038-spark2-43513 (spark2:43513) with 2 cores
15/06/16 18:36:50 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150616183650-0000/0 on hostPort spark2:43513 with 2 cores, 512.0 MB RAM
15/06/16 18:36:50 INFO AppClient$ClientActor: Executor added: app-20150616183650-0000/1 on worker-20150616174035-spark1-59843 (spark1:59843) with 2 cores
15/06/16 18:36:50 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150616183650-0000/1 on hostPort spark1:59843 with 2 cores, 512.0 MB RAM
15/06/16 18:36:50 INFO AppClient$ClientActor: Executor added: app-20150616183650-0000/2 on worker-20150616174035-spark3-44636 (spark3:44636) with 2 cores
15/06/16 18:36:50 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150616183650-0000/2 on hostPort spark3:44636 with 2 cores, 512.0 MB RAM
15/06/16 18:36:50 INFO AppClient$ClientActor: Executor updated: app-20150616183650-0000/2 is now LOADING
15/06/16 18:36:50 INFO AppClient$ClientActor: Executor updated: app-20150616183650-0000/0 is now LOADING
15/06/16 18:36:50 INFO AppClient$ClientActor: Executor updated: app-20150616183650-0000/0 is now RUNNING
15/06/16 18:36:50 INFO AppClient$ClientActor: Executor updated: app-20150616183650-0000/1 is now RUNNING
15/06/16 18:36:50 INFO AppClient$ClientActor: Executor updated: app-20150616183650-0000/2 is now RUNNING
15/06/16 18:36:50 INFO AppClient$ClientActor: Executor updated: app-20150616183650-0000/1 is now LOADING
15/06/16 18:36:50 INFO NettyBlockTransferService: Server created on 52954
15/06/16 18:36:50 INFO BlockManagerMaster: Trying to register BlockManager
15/06/16 18:36:50 INFO BlockManagerMasterActor: Registering block manager spark1:52954 with 265.1 MB RAM, BlockManagerId(<driver>, spark1, 52954)
15/06/16 18:36:50 INFO BlockManagerMaster: Registered BlockManager
15/06/16 18:36:50 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
15/06/16 18:36:51 INFO MemoryStore: ensureFreeSpace(180608) called with curMem=0, maxMem=278019440
15/06/16 18:36:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 176.4 KB, free 265.0 MB)
15/06/16 18:36:51 INFO MemoryStore: ensureFreeSpace(25432) called with curMem=180608, maxMem=278019440
15/06/16 18:36:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.8 KB, free 264.9 MB)
15/06/16 18:36:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark1:52954 (size: 24.8 KB, free: 265.1 MB)
15/06/16 18:36:51 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/06/16 18:36:51 INFO SparkContext: Created broadcast 0 from textFile at SimpleApp.scala:11
15/06/16 18:36:51 INFO FileInputFormat: Total input paths to process : 1
15/06/16 18:36:51 INFO SparkContext: Starting job: count at SimpleApp.scala:12
15/06/16 18:36:51 INFO DAGScheduler: Got job 0 (count at SimpleApp.scala:12) with 2 output partitions (allowLocal=false)
15/06/16 18:36:51 INFO DAGScheduler: Final stage: Stage 0(count at SimpleApp.scala:12)
15/06/16 18:36:51 INFO DAGScheduler: Parents of final stage: List()
15/06/16 18:36:51 INFO DAGScheduler: Missing parents: List()
15/06/16 18:36:51 INFO DAGScheduler: Submitting Stage 0 (MapPartitionsRDD[2] at filter at SimpleApp.scala:12), which has no missing parents
15/06/16 18:36:52 INFO MemoryStore: ensureFreeSpace(2816) called with curMem=206040, maxMem=278019440
15/06/16 18:36:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 2.8 KB, free 264.9 MB)
15/06/16 18:36:52 INFO MemoryStore: ensureFreeSpace(2039) called with curMem=208856, maxMem=278019440
15/06/16 18:36:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2039.0 B, free 264.9 MB)
15/06/16 18:36:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark1:52954 (size: 2039.0 B, free: 265.1 MB)
15/06/16 18:36:52 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/06/16 18:36:52 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:839
15/06/16 18:36:52 INFO DAGScheduler: Submitting 2 missing tasks from Stage 0 (MapPartitionsRDD[2] at filter at SimpleApp.scala:12)
15/06/16 18:36:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
15/06/16 18:36:52 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@spark3:40970/user/Executor#-1310894947] with ID 2
15/06/16 18:36:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, spark3, PROCESS_LOCAL, 1364 bytes)
15/06/16 18:36:52 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, spark3, PROCESS_LOCAL, 1364 bytes)
15/06/16 18:36:52 INFO BlockManagerMasterActor: Registering block manager spark3:53297 with 265.1 MB RAM, BlockManagerId(2, spark3, 53297)
15/06/16 18:36:52 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@spark2:45832/user/Executor#1213934712] with ID 0
15/06/16 18:36:52 INFO BlockManagerMasterActor: Registering block manager spark2:44954 with 265.1 MB RAM, BlockManagerId(0, spark2, 44954)
15/06/16 18:36:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark3:53297 (size: 2039.0 B, free: 265.1 MB)
15/06/16 18:36:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark3:53297 (size: 24.8 KB, free: 265.1 MB)
15/06/16 18:36:53 INFO BlockManagerInfo: Added rdd_1_0 in memory on spark3:53297 (size: 6.1 KB, free: 265.1 MB)
15/06/16 18:36:53 INFO BlockManagerInfo: Added rdd_1_1 in memory on spark3:53297 (size: 5.3 KB, free: 265.1 MB)
15/06/16 18:36:53 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1242 ms on spark3 (1/2)
15/06/16 18:36:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1299 ms on spark3 (2/2)
15/06/16 18:36:53 INFO DAGScheduler: Stage 0 (count at SimpleApp.scala:12) finished in 1.555 s
15/06/16 18:36:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/06/16 18:36:53 INFO DAGScheduler: Job 0 finished: count at SimpleApp.scala:12, took 1.850028 s
15/06/16 18:36:53 INFO SparkContext: Starting job: count at SimpleApp.scala:13
15/06/16 18:36:53 INFO DAGScheduler: Got job 1 (count at SimpleApp.scala:13) with 2 output partitions (allowLocal=false)
15/06/16 18:36:53 INFO DAGScheduler: Final stage: Stage 1(count at SimpleApp.scala:13)
15/06/16 18:36:53 INFO DAGScheduler: Parents of final stage: List()
15/06/16 18:36:53 INFO DAGScheduler: Missing parents: List()
15/06/16 18:36:53 INFO DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[3] at filter at SimpleApp.scala:13), which has no missing parents
15/06/16 18:36:53 INFO MemoryStore: ensureFreeSpace(2816) called with curMem=210895, maxMem=278019440
15/06/16 18:36:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.8 KB, free 264.9 MB)
15/06/16 18:36:53 INFO MemoryStore: ensureFreeSpace(2039) called with curMem=213711, maxMem=278019440
15/06/16 18:36:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2039.0 B, free 264.9 MB)
15/06/16 18:36:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark1:52954 (size: 2039.0 B, free: 265.1 MB)
15/06/16 18:36:53 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/06/16 18:36:53 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:839
15/06/16 18:36:53 INFO DAGScheduler: Submitting 2 missing tasks from Stage 1 (MapPartitionsRDD[3] at filter at SimpleApp.scala:13)
15/06/16 18:36:53 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
15/06/16 18:36:53 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, spark3, PROCESS_LOCAL, 1364 bytes)
15/06/16 18:36:53 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, spark3, PROCESS_LOCAL, 1364 bytes)
15/06/16 18:36:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark3:53297 (size: 2039.0 B, free: 265.1 MB)
15/06/16 18:36:53 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 121 ms on spark3 (1/2)
15/06/16 18:36:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 123 ms on spark3 (2/2)
15/06/16 18:36:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/06/16 18:36:53 INFO DAGScheduler: Stage 1 (count at SimpleApp.scala:13) finished in 0.125 s
15/06/16 18:36:53 INFO DAGScheduler: Job 1 finished: count at SimpleApp.scala:13, took 0.165764 s
Lines with a: 60, Lines with b: 29
